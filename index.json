[{"uri":"https://thanhduy2307.github.io/AWS/5-workshop/5.2-auth-setup/","title":"Configuring Google Cloud &amp; Amazon Cognito","tags":[],"description":"","content":" ‚öôÔ∏è Objective: Set up a Google Cloud Platform project to obtain OAuth credentials and configure Amazon Cognito User Pool as the centralized identity management system.\n1. Configuring Google Cloud Platform (GCP) To allow users to sign in with Gmail, we first need to create a project on Google Cloud and request OAuth 2.0 credentials.\nStep 1: Create an OAuth Client ID Go to Credentials ‚Üí Create Credentials ‚Üí OAuth client ID.\nApplication type: Web application Authorized redirect URIs: This is the address where Google will return the token after successful authentication. (This value will be obtained from the Amazon Cognito Domain in the next section.) Screenshot:\nFigure 5.2.2: Creating OAuth Client ID and Client Secret.\nNote: Make sure to save the Client ID and Client Secret for later use in the Cognito configuration.\n2. Configuring Amazon Cognito User Pool After obtaining credentials from Google, proceed to AWS Console to configure a User Pool.\nStep 1: Create User Pool \u0026amp; Identity Provider In the Amazon Cognito interface, create a new User Pool. Under Sign-in experience, select Federated identity providers and choose Google.\nFill in the Client ID and Client Secret obtained from Google Cloud.\nScreenshot:\nFigure 5.2.3: Entering Google authentication details into Cognito.\nStep 2: Configure App Client \u0026amp; Domain Under App integration:\nDomain: Create a Cognito Domain. This domain will be used as the Authorized redirect URI back in Google Cloud. App Client Settings: Allowed callback URLs: Your application‚Äôs frontend URL. OAuth 2.0 Grant Types: Select Authorization code grant. OpenID Connect scopes: Select email, openid, profile. Screenshot:\nFigure 5.2.5: Configuring Redirect URL and OAuth Scopes.\n3. Testing the Configuration (Hosted UI) To verify your configuration, open the Cognito-hosted Hosted UI.\nIf the \u0026ldquo;Continue with Google\u0026rdquo; button appears and functions correctly, the setup is successful.\nScreenshot:\nFigure 5.2.6: Login interface with Google successfully integrated.\n"},{"uri":"https://thanhduy2307.github.io/AWS/5-workshop/5.5-frontend/5.5.1-api/","title":"Create API Gateway &amp; Authentication","tags":[],"description":"","content":" üõ°Ô∏è Goal: Create an HTTP API to consolidate multiple Lambda functions into a single endpoint and secure it using a Cognito Authorizer.\nStep 1: Create HTTP API Go to AWS Console \u0026gt; API Gateway. Choose HTTP API (low cost, high performance) \u0026gt; Click Build. API name: AuroraAPI. Click Next and leave the Integrations blank (we will add them later). Stage: Keep the default $default (Auto-deploy). Click Create. Illustration: Step 2: Connect Lambda (Integrations) We need to declare which Lambda functions this API will point to (created in section 5.4).\nGo to Integrations \u0026gt; Manage integrations \u0026gt; Create. Choose Lambda function. Select the function auroratimeEvent (or other functions you created). Repeat for other functions (auroratimeTodo, etc.). Illustration: Step 3: Create Routes Go to Routes \u0026gt; Create. Define API paths: POST /events -\u0026gt; Select integration auroratimeEvent GET /events -\u0026gt; Select integration auroratimeEvent POST /todos -\u0026gt; Select integration auroratimeTodo ‚Ä¶ (Note: Action logic can be handled inside the Lambda code or further divided by more detailed routes.) Illustration: Step 4: Configure CORS (Important) To allow the Frontend (Amplify) to call the API, enable CORS:\nGo to CORS. Access-Control-Allow-Origin: *. Access-Control-Allow-Methods: GET, POST, PUT, DELETE, OPTIONS. Access-Control-Allow-Headers: Content-Type, Authorization. Click Save. Illustration: Step 5: Configure Authentication (JWT Authorizer) This step secures the API. Only requests with a Cognito token are allowed.\nGo to Authorization \u0026gt; Manage authorizers tab \u0026gt; Create. Authorizer type: JWT. Name: CognitoAuth. Identity source: $request.header.Authorization. Issuer URL: https://cognito-idp.ap-southeast-1.amazonaws.com/ap-southeast-1_TryyHPjm0 (replace with your User Pool ID). Audience: 5dct7sk93a0unassp7komfpidq Click Create. Attach Authorizer: Go back to the Attach authorizers to routes tab, select routes (/events, /todos, ‚Ä¶) and assign CognitoAuth to them. Illustration: "},{"uri":"https://thanhduy2307.github.io/AWS/5-workshop/5.4-backend-logic/5.4.2-todo/","title":"CRUD Todo (Tasks)","tags":[],"description":"","content":" üìÖ Function: This function handles all Create, Read, Update, and Delete (CRUD) operations for the user\u0026rsquo;s task list.\nStep 1: Create Lambda Function Function name: auroratimeTodo Runtime: Node.js 18.x Description: API that handles CRUD operations for the Todo table. Image: Step 2: Configure IAM Role (Full Access to Todo) We need to grant full CloudWatch and DynamoDB access to the todo table.\nJSON Policy (CloudWatch Logs):\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:ap-southeast-1:080563425614:*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogTodo\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:logs:ap-southeast-1:080563425614:log-group:/aws/lambda/auroraTimeTodo:*\u0026#34; ] } ] } ```json ** JSON Policy (CRUD permissions for DynamoDB todo table) { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCRUDOnTodoTable\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34;, \u0026#34;dynamodb:DeleteItem\u0026#34;, \u0026#34;dynamodb:Query\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:ap-southeast-1:080563425614:table/todo\u0026#34; } ] } Step 3: Implement the Logic (Node.js) Return to the Lambda Function interface and begin writing your Node.js code to handle the CRUD operations.\nH√¨nh ·∫£nh: After completing, click Deploy to save your changes.\n"},{"uri":"https://thanhduy2307.github.io/AWS/5-workshop/5.4-backend-logic/5.4.1-event/","title":"Event CRUD (Event Handler)","tags":[],"description":"","content":" üìÖ Function: This function handles all Create, Read, Update, and Delete (CRUD) operations for the user\u0026rsquo;s schedules/events.\nStep 1: Create Lambda Function Function Name: auroraTimeEvent Runtime: Node.js 18.x Description: API for handling CRUD operations for the Events table. Image: Step 2: Configure IAM Role (Full Access to Events) We need to grant full read/write permissions on the events table.\nJSON Policy (CloudWatch Logs permissions):\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:ap-southeast-1:080563425614:*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:logs:ap-southeast-1:080563425614:log-group:/aws/lambda/auroraTimeEvent:*\u0026#34; ] } ] } ** JSON Policy (CRUD permissions for DynamoDB events table) { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCRUDOnEventsTable\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34;, \u0026#34;dynamodb:DeleteItem\u0026#34;, \u0026#34;dynamodb:Query\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:ap-southeast-1:080563425614:table/events\u0026#34; } ] } Step 3: Processing code (Node.js) Back to the Lambda Function interface, we will write Node.js code to process CRUD operations.\nH√¨nh ·∫£nh: After completing, click Deploy to save.\n"},{"uri":"https://thanhduy2307.github.io/AWS/5-workshop/5.4-backend-logic/5.4.4-resend/","title":"Resend &amp; Route 53 Configuration","tags":[],"description":"","content":" üìß Goal: Before writing the email-sending logic, we need to verify our domain (Domain Verification) to ensure outgoing emails do not land in Spam. We will connect Resend with AWS Route 53.\nStep 1: Add Domain to Resend Go to the Resend Dashboard. Click Add Domain. Enter your domain: auroratime.click. Select Region. Click Add. Resend will provide you with three types of DNS records (MX, SPF, DKIM). Illustration: Step 2: Configure DNS in AWS Route 53 We need to copy the DNS records from Resend and add them to Route 53.\nGo to AWS Console \u0026gt; Route 53 \u0026gt; Hosted zones. Select your domain. Click Create record. Create the MX record (Mail Exchange): Record name: (Leave empty or use the value provided by Resend) Record type: MX Value: Copy from Resend Create the TXT records (SPF \u0026amp; DKIM): Do the same for each TXT record required by Resend. Note: If the Record name ends with your domain, in Route 53 you only need to enter the prefix (e.g., bounces), because Route 53 will automatically append the domain. Illustration: Step 3: Verify and Get API Key Return to Resend and click Verify DNS Records. Wait around 1‚Äì5 minutes until the status turns Verified (Green). Go to API Keys \u0026gt; Create API Key. Name your key and select Sending access. Copy and store this API Key securely Image:\nImage: Illustration: üí° Tip: This DNS configuration improves your domain\u0026rsquo;s reputation, ensuring that system emails from Aurora land in the Inbox instead of Spam.\n"},{"uri":"https://thanhduy2307.github.io/AWS/5-workshop/5.1-architecture/","title":"System Architecture &amp; Authentication Flow","tags":[],"description":"","content":" üìã Overview: This section describes the high-level architecture of the Aurora system and the authentication flow using Google OAuth 2.0 integrated with Amazon Cognito.\n1. High-Level System Architecture The Aurora system is built entirely on a Serverless architecture on AWS, optimizing both scalability and operational cost.\nIt integrates with Google Cloud Platform to provide a seamless Single Sign-On (SSO) experience via Google authentication.\nMain Components: Frontend (Client):\nA Web App (SPA) where users interact with the system, view schedules, and create tasks.\nAuthentication Layer:\nGoogle Cloud Project: Provides OAuth 2.0 Client ID/Secret to authenticate Gmail users. Amazon Cognito: Acts as the federated Identity Provider (IdP), managing the User Pool and issuing temporary AWS credentials to the Frontend. Backend Logic (Compute):\nAWS Lambda: Hosts business logic functions (Create Event, Update Tasks, Trigger Email Notifications). Database:\nAmazon DynamoDB: Stores Events and Daily Worklogs.\nUses UserId as the Partition Key to isolate each user\u0026rsquo;s data securely. Notification Service:\nEmail Sending Logic: Triggered by Lambda when a new event is created or when the scheduled time arrives.\nSends HTML-formatted emails using SES or a custom Email API. 2. Authentication Flow This flow ensures that only authenticated users can access their personal data.\nAurora uses Cognito Federated Identities combined with Google OAuth 2.0.\nStep-by-step Process: User Login:\nThe user clicks ‚ÄúSign in with Google‚Äù on the Frontend.\nGoogle OAuth:\nThe user is redirected to Google‚Äôs login page.\nAfter a successful login, Google returns an Id Token (JWT).\nToken Exchange:\nThe Frontend sends the Id Token to Amazon Cognito.\nVerification \u0026amp; Session Handling:\nCognito validates the Token with Google. If the token is valid:\nCognito creates/updates the corresponding user profile in the User Pool. Cognito returns temporary AWS credentials (Access Key, Secret Key, Session Token) to the Frontend. Authorized Requests:\nThe Frontend uses these credentials to access API resources (via API Gateway or directly invoking Lambda/DynamoDB through AWS SDK) with permissions defined in IAM Roles.\n3. Data Flow: Creating an Event \u0026amp; Sending Email Notifications When a user creates a new event (e.g., ‚ÄúTeam Meeting at 9:00 AM‚Äù), the data flow proceeds as follows:\nFrontend ‚Üí API Request:\nThe frontend sends a POST request containing the event details to the API endpoint.\nAWS Lambda Trigger:\nThe Lambda function is invoked and performs:\nInput validation Writing event data to DynamoDB (Table: AuroraEvents) Email Notification Trigger:\nAfter writing to DynamoDB, Lambda triggers the email-sending logic. The system generates an HTML email body. The Email Service (SES/Gmail API/Resend) sends the message to the user‚Äôs inbox. üí° Highlight: Integrating Google Login eliminates the need for users to manage an additional password, while benefiting from Google‚Äôs built-in two-factor authentication for enhanced security.\n"},{"uri":"https://thanhduy2307.github.io/AWS/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Truong Thanh Duy\nPhone Number: 0357896187\nEmail: duyttse183779@fpt.edu.vn\nUniversity: FPT University\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 12/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://thanhduy2307.github.io/AWS/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Typically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Doing task A\u0026hellip;\nWeek 3: Doing task B\u0026hellip;\nWeek 4: Doing task C\u0026hellip;\nWeek 5: Doing task D\u0026hellip;\nWeek 6: Doing task E\u0026hellip;\nWeek 7: Doing task G\u0026hellip;\nWeek 8: Doing task H\u0026hellip;\nWeek 9: Doing task I\u0026hellip;\nWeek 10: Doing task L\u0026hellip;\nWeek 11: Doing task M\u0026hellip;\nWeek 12: Doing task N\u0026hellip;\n"},{"uri":"https://thanhduy2307.github.io/AWS/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":" Note: The article below is an interpreted translation of an AWS Database Blog.\nPlease do not copy it verbatim into any official report.\nOptimizing Code Conversion \u0026amp; Testing from Microsoft SQL Server and Oracle to PostgreSQL Using Amazon Bedrock Authors: Swanand Kshirsagar, Jose Amado-Blanco, Viswanatha Shastry Medipalli\nPublished: June 2, 2025\nCategories: Amazon Aurora, Amazon RDS, AWS Bedrock, Schema Conversion, PostgreSQL, Technical How-to\nOrganizations today are modernizing their database infrastructure by migrating from legacy engines such as Microsoft SQL Server and Oracle to PostgreSQL, an open-source engine that provides cost benefits, flexibility, and strong scalability.\nMigrating to PostgreSQL not only reduces licensing costs, but also enables innovation through PostgreSQL‚Äôs rich feature set.\nThis blog explains how Amazon Bedrock‚Äôs generative AI can accelerate and simplify the process of converting and testing database code during migration.\nBecause Amazon Q Developer is built on top of Bedrock models, the same approach can also be applied when using Amazon Q Developer.\nChallenges in Heterogeneous Database Migration Migrating from SQL Server or Oracle to PostgreSQL typically involves:\n1. Schema Conversion Translating tables, views, constraints, indexes, and data types into formats compatible with PostgreSQL.\n2. Business Logic Conversion Transforming stored procedures, triggers, functions, and error-handling logic into PL/pgSQL.\n3. Data Migration Executing ETL processes while ensuring accuracy, consistency, and type compatibility.\n4. Application Code Updates Adjusting SQL queries or ORM configuration to work correctly with PostgreSQL.\n5. Performance Tuning Optimizing SQL, indexing, and execution plans in the new PostgreSQL engine.\nThese tasks require significant manual effort and expertise‚Äîand introduce risk if errors go undetected.\nHow Amazon Bedrock Helps Amazon Bedrock can automate and optimize multiple migration steps using generative AI, reducing effort, time, and risk.\nAutomated Schema \u0026amp; Code Conversion Bedrock can analyze SQL Server or Oracle code and produce PostgreSQL-compatible:\nTable definitions Stored procedures \u0026amp; functions Triggers Exception handling logic AI-Driven Data Transformation Bedrock models identify necessary adjustments for dates, numeric types, money types, and structural differences to match PostgreSQL.\nCode Compatibility Insights Bedrock detects:\nUnsupported SQL Server functions Oracle PL/SQL constructs Proprietary syntax Transaction behavior differences ‚Ä¶and suggests equivalent PostgreSQL alternatives.\nIntelligent Testing \u0026amp; Validation Bedrock can generate:- Test cases\nValidation scripts Code coverage analysis This ensures the converted code meets functional and performance requirements before production deployment.\nPrompt Engineering for Accuracy Well-designed prompts can force Bedrock to:\nPreserve naming conventions Follow PostgreSQL best practices Maintain business logic accuracy Reduce ambiguity Iterative prompt refinement improves conversion accuracy and reduces manual rework.\nExample: Converting SQL Server Procedure to PostgreSQL Function Using Amazon Bedrock:\nOpen Amazon Bedrock Console ‚Üí Playgrounds ‚Üí Chat/Text Select Anthropic ‚Üí Claude 3.5 Sonnet Paste prompt asking Bedrock to convert SQL Server procedure to PostgreSQL function. Bedrock generates:\nA PostgreSQL plpgsql function Equivalent logic Error handling using PostgreSQL conventions A full set of test cases covering: Valid updates Invalid parameters No eligible employees Max increase cap Percentage-based increase Inactive employees Insufficient tenure Multiple employees If the initial response changes naming conventions (e.g., lowercase or underscores), you can request Bedrock to regenerate while preserving original naming.\nTest Coverage Analysis Bedrock can also analyze the generated test cases and provide coverage breakdown, indicating which parts of the code are tested:\nInput validation Salary calculation Eligibility rules Update operations Logging Exception handling Return result logic Suggested additional tests may include:\nEdge date conditions Large numeric values Simulated database errors For full precision coverage, tools like pgTAP are recommended.\nGenerating Supporting Database Objects Upon request, Bedrock can generate:\nCREATE TABLE scripts (Employees, SalaryUpdateLog) Sample testing data Environment initialization scripts This helps you fully reproduce the example function locally during migration testing.\nAutomating Test Execution Across Source \u0026amp; Target You can integrate Bedrock APIs into a custom application to:\nRun test cases against SQL Server (source) and PostgreSQL (target) Compare results automatically Identify logic discrepancies Measure performance differences Generate migration summary reports This eliminates manual verification and accelerates modernization.\nAWS Database Migration Service (AWS DMS) Schema Conversion accelerators can also support migration and code conversion workflows.\nConclusion AWS provides a powerful set of tools and services to modernize enterprise databases.\nThis blog demonstrated how Amazon Bedrock can:\nConvert SQL Server/Oracle code to PostgreSQL\nGenerate complete test cases\nAnalyze code coverage\nProduce supporting schema and test data\nImprove accuracy through prompt refinement By incorporating Bedrock into your migration strategy, organizations can significantly reduce:\nMigration time\nOperational cost\nRisk of logic or compatibility errors\nTo get started with Amazon Bedrock, refer to Getting Started with Amazon Bedrock in AWS documentation.\n"},{"uri":"https://thanhduy2307.github.io/AWS/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS infrastructure and how to use the AWS Management Console. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of the internship unit rules and regulations 08/09/2025 08/09/2025 https://policies.fcjuni.com/ 3 - Draw AWS architecture using draw.io 09/09/2025 09/09/2025 https://youtu.be/l8isyDe-GwY?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i 4 - Watch workshop tutorials 10/09/2025 10/09/2025 https://www.youtube.com/watch?v=mXRqgMr_97U\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=3\u0026pp=iAQB 5 - Learn basic theories about AWS infrastructure 11/09/2025 11/09/2025 6 - Practice: Set up Account and get familiar with AWS console 12/09/2025 12/09/2025 https://000001.awsstudygroup.com/vi/ Week 1 Achievements: Understood what AWS is. Successfully created and configured an AWS Free Tier account. Became familiar with the AWS Management Console and learned how to find, access, and use services via the web interface. "},{"uri":"https://thanhduy2307.github.io/AWS/5-workshop/5.5-frontend/5.5.2-amplify/-index/","title":"Deploy Amplify &amp; Integration","tags":[],"description":"","content":"1. Deploy Frontend with Amplify Go to AWS Amplify \u0026gt; Create new app. Select GitHub \u0026gt; Select the Repository containing your React code. Build settings: Amplify automatically detects npm run build. Click Save and Deploy. Illustration: "},{"uri":"https://thanhduy2307.github.io/AWS/4-eventparticipated/4.1-event1/","title":"Exploring Agentic AI ‚Äì Amazon QuickSuite Workshop","tags":[],"description":"","content":"Exploring Agentic AI ‚Äì Amazon QuickSuite Workshop - Date: November 7, 2025\n- Location: AWS Vietnam Office, Bitexco Financial Tower, Ho Chi Minh City\nEvent Overview A special workshop focused on the transition from passive Generative AI to autonomous Agentic AI. The event featured the first live demonstration of Amazon QuickSuite in Vietnam and introduced the AWS LIFT Program designed to reduce financial barriers to adoption.\nKey Objectives:\nDefine Agentic AI: Clarify the concept of autonomous AI agents capable of reasoning and executing tasks. Introduce Amazon QuickSuite: Showcase the unified platform combining data visualization (QuickSight) and generative AI (QuickSuite Q). Hands-on Learning Support: Provide a real-world environment for building AI concepts with expert guidance. Drive Adoption: Offer USD 80,000 in credits through the AWS LIFT Program to accelerate R\u0026amp;D. Key Learnings and Insights Focus on Autonomy: The design goal of Agentic AI is to build systems that act on behalf of users, not just provide information. Ecosystem Approach Is Critical: Effective agents require a connected network of tools‚Äîlike the ecosystem provided by QuickSuite‚Äîthat links data sources with action logic. Early Adoption Creates Advantage: Mastering tools like QuickSuite before they become widespread offers a significant competitive edge. Funding Fuels Innovation: Financial incentives such as the LIFT program enable companies to experiment and innovate faster. Application to Work Explore QuickSuite for Analytics: Investigate integrating QuickSight and QuickSuite Q to create ‚ÄúAnalytics Agents‚Äù capable of automating reporting and data analysis. Secure Funding for R\u0026amp;D: Apply for the AWS LIFT Program to secure credits for upcoming research and development projects related to AI. "},{"uri":"https://thanhduy2307.github.io/AWS/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Understand the AWS EC2 service, IAM permissions, and VPC concepts. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Manage costs with AWS Budget 15/09/2025 15/09/2025 https://000007.awsstudygroup.com/vi/ 3 - Request support from AWS Support - Manage access permissions with AWS Identity and Access Management (IAM) 16/09/2025 16/09/2025 https://000009.awsstudygroup.com/vi/ https://000002.awsstudygroup.com/vi/ 4 - Basic networking knowledge with Amazon Virtual Private Cloud (VPC) 17/09/2025 17/09/2025 https://000003.awsstudygroup.com/vi/ 5 - Knowledge about virtual servers with Amazon Elastic Compute Cloud (EC2) 18/09/2025 18/09/2025 https://000004.awsstudygroup.com/vi/ 6 - Practice: Lab on virtual servers with Amazon Elastic Compute Cloud (EC2) 19/09/2025 19/09/2025 https://000004.awsstudygroup.com/vi/ Week 2 Achievements: üöÄ Cost Management: Understood how to set up AWS Budget to monitor and receive cost alerts, ensuring service usage stays within the Free Tier limits. Identity and Access Management (IAM): Gained a clear understanding of the roles of IAM Users, Groups, and Policies. Practiced creating a new IAM User with restricted permissions (least privilege) for resource management. Networking (VPC): Mastered the fundamental components of a virtual private cloud on AWS, including: VPC, Subnets (Public/Private), Route Tables, and Internet Gateway. Understood the principles of network isolation and traffic routing within a VPC. Virtual Servers (EC2): Understood key EC2 concepts: Instance Types, AMI, and EBS. Successfully practiced Launching and Terminating an EC2 instance (Free Tier). Learned how to use Security Groups to control access to EC2 instances. "},{"uri":"https://thanhduy2307.github.io/AWS/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Structured JSON Logging for .NET Lambda Functions Author: Norm Johanson\nPublished: November 7, 2024\nCategories: .NET, AWS Lambda, Developer Tools\nNorm Johanson is a software developer with over 25 years of experience building many types of applications. Since 2010, he has worked at AWS, focusing on improving the experience for .NET developers. You can find him on Twitter @socketnorm and GitHub @normj.\nAWS has announced structured JSON logging support for the .NET managed runtime in AWS Lambda. This enhancement brings the .NET managed runtime in line with previously released Lambda logging controls, allowing developers to change the logging format and log levels through the Lambda API.\nFormatting log messages as JSON documents makes it significantly easier to search, filter, and analyze Lambda logs‚Äîespecially when monitoring, debugging, or creating automated alarms based on specific log fields.\nEnabling Structured JSON Logging By default, Lambda logs use plain text format.\nYou can switch a function to JSON log format using AWS tooling, including:\nAWS Lambda Console .NET CLI tool Amazon.Lambda.Tools AWS Tools for PowerShell AWS CLI In the Lambda Console, the logging configuration is available under:\nConfiguration ‚Üí Monitoring and operations tools ‚Üí Logging configuration\nBeginning with Amazon.Lambda.Tools version 5.11.0, new command-line switches were added:\n--log-format --log-application-level --log-system-level --log-group Example deployment enabling JSON logging:\ndotnet tool install -g amazon.lambda.tools dotnet lambda deploy-function \u0026lt;function-name\u0026gt; --log-format JSON "},{"uri":"https://thanhduy2307.github.io/AWS/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Aurora Time Unified AWS Serverless Solution for Personal Time Management 1. Executive Summary This proposal presents the implementation plan for Aurora Time, a time management application on the AWS platform, simple and focused on scheduling features, to address the complexity and operational costs of current solutions. Aurora Time will leverage AWS serverless and managed services to ensure high scalability, reliability, and cost optimization, delivering rapid return on investment (ROI) through reduced infrastructure management costs.\n2. Problem Statement Current Problem Individuals struggle to manage daily commitments because schedules are scattered (notes, phones) leading to confusion and missed tasks. Current tools are often too complex, overloaded with work features, and unsuitable for the need for quick and simple scheduling in personal life. Aurora Time solves this by providing a centralized, minimalist, and intuitive platform to easily track habits and important milestones.\nSolution The platform uses Amazon S3 combined with Amazon CloudFront to store and distribute web applications, using AWS Amplify for rapid development and deployment. Amazon API Gateway and AWS Lambda serve as the Backend processing layer to handle event CRUD requests. Data is stored in Amazon DynamoDB to ensure fast access speed and low latency. Amazon Cognito ensures secure authentication and authorization for each individual user. Amazon EventBridge and Amazon SES are used to trigger and send scheduled reminder notifications. Similar to other calendar applications, users can create and edit personal schedules, but this platform operates at a more minimalist scale and serves the purpose of daily personal time management. Key features include intuitive scheduling interface, customizable reminders, and low operational costs.\nBenefits and Return on Investment (ROI) The Aurora Time solution creates a solid foundation for individual users to centralize all schedules while providing a cost-saving Serverless architecture model for easy feature expansion. The platform reduces schedule fragmentation and minimizes missed important commitments through a centralized, simple reminder system, simplifying personal time management and improving work/life balance.\nEstimated monthly infrastructure costs of $16 - $50 USD, totaling approximately $192 - $600 USD for 12 months (depending on number of users and requests). All development components are based on Serverless, incurring no hardware purchases or 24/7 virtual server rental costs. The payback period is estimated at under 6 months thanks to significant time savings in searching and manually arranging schedules, and the extremely low operational costs of the AWS Serverless architecture.\n3. Solution Architecture The platform applies AWS Serverless architecture to manage personal schedule and event data, with the capability to easily scale from a single user to millions of individual users. API requests are received through Amazon API Gateway and processed by AWS Lambda, while data is stored in Amazon DynamoDB to ensure fast query speed and low latency. Amazon EventBridge handles reminder scheduling logic, triggers Lambda, and sends notifications. AWS Amplify provides an intuitive web/mobile interface, secured by Amazon Cognito to safely manage access permissions for each user.\nAWS Services Used AWS Lambda: Processes business logic for event CRUD operations and triggers scheduled reminder tasks. Amazon API Gateway: Provides secure RESTful API interface for communication with web applications. Amazon DynamoDB: Stores event data, schedules, and user information. Amazon S3 and CloudFront: Stores and distributes static content of Frontend applications. Amazon EventBridge: Schedules and triggers automatic reminder events at user-defined times. AWS Amplify: Stores and provides intuitive web interface. Amazon Cognito: Manages access permissions and secure authentication for individual users. Component Design User Interface: AWS Amplify hosts web applications (planned to use React) providing intuitive scheduling interface, schedule viewing, and reminder settings. User Authentication: Amazon Cognito manages individual user accounts, issuing secure authorization tokens for Backend API access. API Request Reception: Amazon API Gateway receives and routes authenticated requests (e.g., create event, query schedule) to corresponding Lambda functions. Backend Logic Processing: AWS Lambda processes and executes business logic. Meanwhile, other Lambda functions are triggered by EventBridge to send reminders. Data Storage: Amazon DynamoDB stores primary data (Schedules, Events, User Information) in NoSQL format, ensuring fast access and flexibility. Reminder Scheduling: Amazon EventBridge schedules and triggers events at user-defined times, ensuring reminder features operate automatically. Web Interface: AWS Amplify hosts a Next.js app for real-time dashboards and analytics. User Management: Amazon Cognito manages user access, allowing up to 5 active accounts. 4. Technical Implementation Implementation Phases\nThe project consists of 2 parts ‚Äî Backend Serverless setup and Frontend User Interface building ‚Äî each part goes through 4 phases:\nResearch and Architecture Design: In-depth research on DynamoDB Data Modeling for schedules and design of verified AWS Serverless architecture (API Gateway, Lambda, EventBridge). (Timeline: Month 1) Cost Calculation and Feasibility Check: Use AWS Pricing Calculator to estimate actual Serverless operational costs and verify feasibility of authentication (Cognito) and storage (DynamoDB) flows. (Timeline: Month 1) Architecture Adjustment for Cost/Solution Optimization: Fine-tune Lambda parameters (e.g., memory, timeout) and DynamoDB (e.g., RCU/WCU) to ensure highest cost efficiency and best performance for individual users. (Timeline: Month 2) Development, Testing, Deployment: Program Lambda functions, set up CI/CD Pipeline with CodePipeline/CodeBuild/CloudFormation, and develop Frontend applications (React). Then conduct Beta testing and go live. (Timeline: Month 2-3) Technical Requirements\n1. Backend Requirements (Serverless)\nCore Services: In-depth knowledge of AWS Lambda (Node.js), Amazon DynamoDB, Amazon API Gateway, and Amazon Cognito. Event Management: Proficiency in Amazon EventBridge to schedule and trigger Lambda functions sending reminders via Amazon SES/SNS. 2. Frontend Requirements\nInterface: Practical knowledge of AWS Amplify to host React applications and connect to API Gateway Optimization: Leverage React\u0026rsquo;s processing capabilities to reduce load on Lambda functions 5. Roadmap \u0026amp; Deployment Milestones Internship (Month 1‚Äì3): Month 1: Learn AWS and upgrade hardware. Month 2: Design and adjust architecture. Month 3: Deploy, test, and go live. Post-Deployment: Further research over 1 year. 6. Budget Estimation AWS Service Estimated Usage Unit Pricing \u0026amp; Free Tier Cost/Month (USD) AWS Amplify Static web hosting $0.023/GB + $0.15/GB out 0.35 S3 Static files, backup $0.023/GB 0.05 CloudFront Content CDN (20GB) $0.085/GB 1.70 API Gateway 30,000 requests $3.50/1 million req, 1M free 0.11 AWS Lambda 1 million requests $0.21/1 million, 1M free 0.00 (Free) DynamoDB ~1GB data (events) $0/25GB, first 25GB free 0.11 Amazon Cognito \u0026lt;1000 active users Free up to 50k users/month 0.00 (Free) EventBridge 100,000 scheduled events $1/million events 0.10 CloudWatch Logs 1GB log/month $0.50/GB ingest + $0.03/GB storage 0.10 CI/CD Pipeline + Build 20 build/run 100 min free/month 0.00 (Free) TOTAL 2.57 USD 7. Risk Assessment Risk Matrix\nNetwork disconnection/High latency: Medium impact, medium probability. DynamoDB design error: High impact, medium probability. Serverless costs exceed budget: Medium impact, low probability. Reminder system failure: High impact, low probability. Mitigation Strategies\nNetwork disconnection/High latency: Optimize Frontend (stored on S3/CloudFront) to ensure fast loading speeds, use Client-side Caching so users can still view recent schedules when network is lost. DynamoDB design error: Conduct strict Proof of Concept (POC) and Load Testing for data models. Use optimized Global Secondary Index (GSI) to avoid querying entire table and minimize RCU/WCU costs. Serverless costs exceed budget: Set up AWS Budgets with proactive alerts (Email/SNS) when spending approaches threshold. Regularly check AWS Cost Explorer and optimize Lambda resources. Reminder system failure: Closely monitor EventBridge and Lambda reminder processing through CloudWatch Alarms to detect errors immediately. Implement Retry Logic for critical Lambda functions. Contingency Plan\nAWS Incident: Since the application is only for individuals, if AWS encounters an incident, the system will be configured to automatically Rollback to the latest code version (via CodePipeline) or re-deploy configuration (via CloudFormation). Data Loss: Set up DynamoDB Backup and Restore at regular intervals to quickly recover event data in case of system failure or user error. Cost Issues: Use CloudFormation to restore resource configuration (such as RCU/WCU) to proven low-cost state. 8. Expected Results User Experience Improvement: Provide intuitive scheduling interface and real-time notifications (Real-time notifications) replacing manual note-taking and schedule tracking processes. The system is designed for easy scaling, serving from a single user to millions of individual users.\nLong-term Value and Reusability: Create a solid Serverless technical platform that can maintain operation at extremely low costs for many years. This architecture can be reused for personal application projects or other expansion features in the future (e.g., habit tracking, simple personal task management).\nSuccessful Deployment: Successfully deploy the entire approved Serverless architecture, including automated CI/CD and core features of Aurora Time within the planned timeframe.\n"},{"uri":"https://thanhduy2307.github.io/AWS/5-workshop/5.3-database/","title":"Database Design (Events &amp; Tasks)","tags":[],"description":"","content":" üóÑÔ∏è Objective: Design a NoSQL database using Amazon DynamoDB to store Events and Todo tasks, optimized for fast retrieval and low operational cost.\n1. Why Choose Amazon DynamoDB? With Aurora‚Äôs Serverless architecture, Amazon DynamoDB is the ideal choice because:\nServerless: No server management required, automatically scales based on workload. High performance: Low latency (single-digit milliseconds), ideal for real-time UI operations. Flexible (Schemaless): Easy to add new fields to Event/Todo items without complex migrations like SQL databases. 2. Schema Design (Data Modeling) The system follows a Per-User Isolation model.\nEvery item is associated with a userId (extracted from Cognito/Google tokens) to ensure data security.\nWe will create three main tables:\nTable 1: AuroraEvents (Stores calendar events) This table stores user events, displayed on the calendar and used for notification scanning.\nPartition Key (PK): userId (String) ‚Äî User identifier Sort Key (SK): eventId (String) ‚Äî UUID of the event Table 2: AuroraTasks (Stores daily tasks) This table stores the user‚Äôs Daily Worklog / Todo list.\nPartition Key (PK): userId (String) Sort Key (SK): todoId (String) ‚Äî UUID of the task Table 3: users (Stores user information) This table contains detailed information about each user.\nPartition Key (PK): userId (String) 3. Steps to Create Tables on AWS Console Below are the steps to create tables using the AWS DynamoDB Console.\nStep 1: Create the Events Table Navigate to DynamoDB ‚Üí Tables ‚Üí Create table.\nTable name: events Partition key: userId (String) Sort key: eventId (String) Screenshot:\nStep 2: Create the Tasks Table Repeat the process to create the Tasks table.\nTable name: todo Partition key: userId (String) Sort key: todoId (String) Screenshot:\nStep 3: Create the Users Table Finally, create the Users table.\nTable name: users Partition key: userId (String) Screenshot:\n"},{"uri":"https://thanhduy2307.github.io/AWS/5-workshop/5.4-backend-logic/5.4.3-sendreminder/","title":"Send Reminder (Notification)","tags":[],"description":"","content":" üîî Function: This function is designed to run on a schedule (e.g., every 5 minutes using EventBridge Scheduler). It scans the database for events happening within the next 15 minutes and sends reminder emails via a third-party API.\nStep 1: Create Lambda Function This function requires a slightly longer execution time since it needs to scan the database and wait for responses from the email API, so we will increase the Timeout.\nFunction name: sendReminder Runtime: Node.js 18.x Illustration:\nFigure 5.4.3.1: Configuring the background job Lambda function.\nStep 2: Configure IAM Role (Full Access to Events) JSON Policy:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:ap-southeast-1:080563425614:*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:logs:ap-southeast-1:080563425614:log-group:/aws/lambda/sendReminderLambda:*\u0026#34; ] } ] } Step 3: Implementation Code (Node.js) Return to the Lambda Function interface, where we will write Node.js code to scan for upcoming events and send reminder emails via a third-party API.\nImage: After completing the code, click Deploy to save it.\n"},{"uri":"https://thanhduy2307.github.io/AWS/4-eventparticipated/4.2-event2/","title":"AWS Cloud Mastery Series #3 - Security Pillar Deep Dive","tags":[],"description":"","content":"AWS Cloud Mastery Series #3 - Security Pillar Deep Dive - Date: December 1, 2025 - Location: AWS Vietnam Office, Bitexco Financial Tower, HCMC\nEvent Overview An in-depth workshop focused on the Security Pillar of the AWS Well-Architected Framework. The event provided knowledge and best practices for securing cloud workloads.\nKey Objectives:\nDeep Dive into the Security Pillar: Analyze the design principles and key areas of security on AWS. Identity and Access Management: Gain a deep understanding of AWS IAM, MFA, and best practices for access control. Data Protection: Explore techniques for encrypting data at-rest and in-transit. Automation and Monitoring: Learn how to use AWS Config, CloudTrail, and Security Hub to monitor and automate security controls. Key Takeaways \u0026amp; Learnings Security is a Shared Responsibility: Understand the shared responsibility model and the customer\u0026rsquo;s role in securing applications in the cloud. Defense in Depth: Apply multiple layers of security to protect resources comprehensively. Automation is Key: Automating security checks and remediation reduces human error and allows for faster responses to threats. Application to Work Re-evaluate IAM Policies: Review and strengthen existing IAM policies according to the principle of least privilege. Implement Security Monitoring: Set up AWS Security Hub for a centralized, comprehensive view of the security posture. Enhance Data Encryption: Ensure all sensitive data is encrypted using AWS KMS. "},{"uri":"https://thanhduy2307.github.io/AWS/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - Getting started with healthcare data lakes: Using microservices This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"},{"uri":"https://thanhduy2307.github.io/AWS/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Understand and practice object storage using Amazon S3. Master Relational Database concepts on AWS through Amazon RDS and Lightsail Database. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Static Website Hosting with Amazon S3 22/09/2025 22/09/2025 https://000057.awsstudygroup.com/vi/ 3 - Basic knowledge of databases with Amazon Relational Database Service (RDS) 23/09/2025 23/09/2025 https://000005.awsstudygroup.com/vi/ 4 - Create an RDS database instance 24/09/2025 24/09/2025 https://000005.awsstudygroup.com/vi/4-create-rds/ 5 - Deploy Lightsail Database 25/09/2025 25/09/2025 https://000045.awsstudygroup.com/vi/1-database/ 6 - Migrate to a larger instance (Database/Lightsail) 26/09/2025 26/09/2025 https://000045.awsstudygroup.com/vi/7-migrate-to-larger-instances/ Week 3 Achievements: üíæ Object Storage (S3): Understood the concepts of S3 Buckets, Objects, and Access Control List (ACLs). Successfully configured an S3 bucket for Static Website Hosting and accessed the website via URL. Learned how to set bucket policies to secure content. Relational Database Service (RDS): Gained foundational knowledge about the benefits of a managed database service like RDS. Understood key RDS features such as Multi-AZ deployment and automated backups. Successfully created and managed an RDS database instance (e.g., MySQL or PostgreSQL). Lightsail Database: Learned about the simplified database offering provided by Amazon Lightsail. Successfully deployed and configured a Lightsail Database instance. Practiced the process of migrating a database to a larger instance size, understanding the scaling process. "},{"uri":"https://thanhduy2307.github.io/AWS/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"How Blocksee Built a Web3 CRM Using Blockchain Data from Amazon Managed Blockchain (AMB) Query Authors: Forrest Colyer \u0026amp; AJ Park\nPublished: March 6, 2024\nCategories: Amazon Managed Blockchain, Blockchain, Customer Solutions, Foundational (100)\nAbout the Authors AJ Park is a Product Manager on the Amazon Managed Blockchain team at AWS.\nWith over 20 years of experience in data protection and storage as a software engineer and product manager, he is passionate about building blockchain and Web3 solutions for customers.\nForrest Colyer leads the Web3/Blockchain Specialist Solutions Architecture team supporting Amazon Managed Blockchain (AMB).\nHe works with customers across all stages of blockchain adoption‚Äîfrom proof-of-concept to production‚Äîproviding technical expertise and strategic direction for implementing impactful blockchain workloads.\nOverview Blocksee provides a Web3-focused CRM and user engagement platform that delivers rich insights for NFT and crypto marketers.\nBy embedding simple code into a website or using APIs, Blocksee enables marketers to collect data about users who interact with digital memberships, event tickets, and promotional assets.\nPowered by AWS services‚Äîincluding Amazon Managed Blockchain (AMB)‚ÄîBlocksee helps brands build:\nLoyalty programs Token-gated content Dynamic customer journeys Automatic wallet creation via email Onramp payments for receiving digital assets (e.g., NFTs) To enable deep analytics on digital asset activity and user interactions, Blocksee requires massive amounts of on-chain data from public blockchains such as Ethereum, including:\nCurrent \u0026amp; historical token balances Ownership of fungible and non-fungible tokens (NFTs) User interactions with Web3 applications \u0026amp; smart contracts Blocksee evaluated multiple technical approaches‚Äîfrom operating its own full blockchain infrastructure to using third-party providers.\nSelf-managed infrastructure proved too costly and operationally heavy due to:\nHigh compute, storage \u0026amp; networking demands Resource-heavy ETL pipelines and indexing operations Significant developer effort that slowed product feature delivery Third-party blockchain data providers were also insufficient, with challenges such as:\nUnreliable uptime Poor data quality Slow performance High and unpredictable pricing Why Blocksee Chose Amazon Managed Blockchain (AMB) Query Blocksee ultimately adopted AMB Query, a unified blockchain data API for multiple public networks.\nBenefits of switching:\n‚úî Higher reliability### ‚úî Lower and predictable cost ‚úî Faster performance ‚úî A single, consistent API for multiple blockchains This allowed Blocksee to shorten product release cycles and reduce engineering overhead.\nA statement from Eric Forst, Co-founder \u0026amp; CEO of Blocksee, highlights the transformation:\n‚ÄúBefore using Amazon Managed Blockchain, our team had to aggregate data from many different RPC providers, resulting in complex API configurations and monitoring overhead.\nAMB changed everything ‚Äî fast and stable access to blockchain data helped us streamline our system and rely on a more trustworthy infrastructure.‚Äù\nHow Blocksee Uses AMB Query When marketers or project managers open Blocksee CRM, token balance data for blockchain addresses (users) is displayed instantly and combined with additional context such as AI-driven behavioral analytics.\nBlocksee uses the ListTokenBalances API from AMB Query to retrieve ERC-20 and ERC-721 token balances.\nThe API allows Blocksee to:\nList all token balances belonging to a wallet or contract List all token holders for a given smart contract List balances for a specific token across all holders ‚≠ê Result: 25% performance improvement\n50% cost reduction\ncompared to Blocksee‚Äôs previous token data retrieval mechanism.\nBlocksee CTO Matt Kotnik shared additional insights:\n‚ÄúWe saw significant load time improvements‚Äîover 25% faster‚Äîusing AMB Query‚Äôs ListTokenBalances API.\nAMB also allows us to scale easily to additional blockchains thanks to Amazon‚Äôs chain-agnostic design.\nThis helps us focus more on our core product and customer relationships while relying on a trusted infrastructure partner.‚Äù\nBecause AMB Query uses a standardized REST API, integrating new blockchains in the future becomes simple.\nThe query syntax remains nearly the same even when fetching data across multiple networks.\nConclusion AMB Query is now available in the US East (N. Virginia) Region, offering:\nSub-second latency Highly scalable access to blockchain balances \u0026amp; historical transactions No need to operate blockchain infrastructure Simple, predictable pricing based on API calls Customers can rely on AMB Query for high-performance, reliable blockchain data that meets the demands of real-world Web3 applications.\nTo learn more:\nExplore AMB Query Documentation Explore other AMB services at Amazon Managed Blockchain "},{"uri":"https://thanhduy2307.github.io/AWS/5-workshop/5.4-backend-logic/","title":"Backend-Logic","tags":[],"description":"","content":" üõ°Ô∏è Goal: Build a Serverless Backend following a 4-step workflow: Create Function ‚Üí Configure Role ‚Üí Grant Database Permissions ‚Üí Implement Logic (Connect to DB \u0026amp; Call Third-Party Email API).\nStep 1: Initialize the Lambda Function First, we create a new Lambda function that will contain the processing logic.\nGo to AWS Console \u0026gt; Lambda \u0026gt; Create function. Function name: (enter your function name). Runtime: Choose Node.js 18.x (or 20.x). Architecture: x86_64. Keep the remaining settings as default and click Create function. Illustration:\nFigure 5.4.1: Creating the Backend processing Lambda function.\nStep 2: Add Policies to the Lambda (Execution Role) When the function is created, AWS automatically generates a basic IAM Role.\nWe need to access this Role and add permissions for writing to the Database.\nIn the Lambda function page, switch to the Configuration tab. Select Permissions on the left panel. Click the Role name under Execution role to open it in the IAM Console. Illustration:\nFigure 5.4.2: Accessing the IAM Role for permission configuration.\nStep 3: Add Policies for Database Access Since we use a third-party Email service (called via a normal HTTP API), we do not need SES permissions.\nWe only need to grant Lambda permissions to work with DynamoDB.\nIn the Role‚Äôs Permissions tab, click Add permissions \u0026gt; Create inline policy. Switch to the JSON mode. Click Next, name the policy AuroraDB_Access_Policy, and then click Create policy. Review your Permissions list to ensure the Role now has access to DynamoDB. Step 4: Write the Lambda Function Code Return to the Lambda Function interface and begin writing your Node.js code. Once completed, click Deploy to save and apply the changes.\n"},{"uri":"https://thanhduy2307.github.io/AWS/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"Event 1 Event Name: Exploring Agentic AI ‚Äì Amazon QuickSuite Workshop\nDate \u0026amp; Time: 09:00, Novembern 7, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AWS Cloud Mastery Series #3 - Security Pillar Deep Dive\nDate \u0026amp; Time: 09:00, December 1, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://thanhduy2307.github.io/AWS/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Understand and practice system monitoring with Amazon CloudWatch. Grasp basic DNS concepts on AWS using Route 53 Resolverand Content Distribution (CloudFront) Explore NoSQL databases (DynamoDB) and advanced operations with the AWS CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - System monitoring with Amazon CloudWatch 29/09/2025 29/09/2025 https://000008.awsstudygroup.com/vi/ 3 - Setting up Hybrid DNS with Route 53 Resolver 30/09/2025 30/09/2025 https://000010.awsstudygroup.com/vi/ 4 - Command line operations with AWS CLI 01/10/2025 01/10/2025 https://000011.awsstudygroup.com/vi/ 5 - NoSQL database with Amazon DynamoDB 02/10/2025 02/10/2025 https://000060.awsstudygroup.com/vi/ 6 -Learning and implementing Content Distribution with Amazon CloudFront 03/10/2025 03/10/2025 https://000060.awsstudygroup.com/vi/ Week 4 Achievements: üìä Monitoring (CloudWatch): Understood the role of CloudWatch in monitoring performance metrics, system utilization, and application logs. Successfully created CloudWatch Alarms based on EC2 metrics (e.g., CPU utilization). Networking (Route 53): Gained foundational knowledge about DNS and the functionality of Route 53. Understood the role of Route 53 Resolver in a Hybrid Cloud environment. Content Distribution (CloudFront): Acquired basic knowledge of Content Delivery Network (CDN) and the functionality of CloudFront. Successfully configured a CloudFront Distribution to accelerate and secure content delivery. NoSQL Database (DynamoDB): Learned the core concepts of NoSQL and the benefits of a managed service like DynamoDB. Understood key DynamoDB components: Tables, Items, and Attributes. Practiced creating a DynamoDB table and performing basic CRUD operations. AWS CLI Proficiency: Performed more complex operations using the AWS CLI, moving beyond basic configuration checks. Used advanced filtering and output formatting techniques with the CLI to retrieve specific resource information (e.g., filtering EC2 instances by tag). "},{"uri":"https://thanhduy2307.github.io/AWS/5-workshop/","title":"Workshop","tags":[],"description":"","content":" üí° Project Information: Aurora is an event-management and automated notification system built on a Serverless architecture.\nProject Aurora: Event Management \u0026amp; Automated Notification System Project Overview Project Aurora is a solution that enables users to manage schedules, create important events, and track daily tasks (Daily Worklog).\nA key highlight of the system is its ability to integrate automated email notifications, reminding users when an event is about to occur.\nThis project focuses on solving the problem of creating schedules and delivering reliable notifications without maintaining traditional servers.\nKey features and services include:\nAmazon Cognito \u0026amp; Google Cloud: Centralized user identity management with Google Sign-In (OAuth 2.0) for secure and simplified authentication. Event \u0026amp; Task Management: Built using Amazon DynamoDB to store event details and daily task statuses. Email Notification System: Integrated with Resend to deliver beautiful HTML-formatted emails to users. Logic Processing: Implemented with AWS Lambda to handle workflow execution whenever a new event is created. Detailed Sections System Architecture \u0026amp; Authentication Flow Google Cloud \u0026amp; Amazon Cognito Configuration Database Design (Events \u0026amp; Tasks) Building API \u0026amp; Email Logic (Lambda) User Interface (Frontend) Final Results \u0026amp; Future Improvements "},{"uri":"https://thanhduy2307.github.io/AWS/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Master the concepts and practice Serverless architecture using AWS Lambda. Learn about user authentication with Amazon Cognito. Build a basic CI/CD pipeline with AWS CodePipeline. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Automation using Serverless with AWS Lambda 06/10/2025 06/10/2025 https://000022.awsstudygroup.com/vi/ 3 - Cross-domain authentication with Amazon Cognito 07/10/2025 07/10/2025 https://000141.awsstudygroup.com/vi/ 4 - CI/CD Pipeline with AWS CodePipeline 08/10/2025 08/10/2025 https://000017.awsstudygroup.com/vi/ 5 - Frontend Development for Serverless API 09/10/2025 09/10/2025 https://000079.awsstudygroup.com/vi/ 6 - Serverless Backend with Lambda, S3, and DynamoDB 10/10/2025 10/10/2025 https://000078.awsstudygroup.com/vi/ Week 5 Achievements: ‚òÅÔ∏è Automation (Lambda): Mastered the Serverless concept and how AWS Lambda operates. Successfully created and deployed a basic Lambda Function. Understood how to configure Triggers (event sources) for Lambda functions. Authentication (Cognito): Learned about the Amazon Cognito service and its role in user identity management. Successfully created and configured a basic Cognito User Pool for managing sign-ups/sign-ins. Development and Deployment (CI/CD): Understood the Continuous Integration/Continuous Delivery (CI/CD) process. Built a simple CI/CD Pipeline using AWS CodePipeline to automate code deployment. End-to-End Serverless Architecture: Understood and practiced building a basic Serverless Backend by integrating Lambda (logic), S3 (static/frontend storage), and DynamoDB (NoSQL database). Gained knowledge on developing a Frontend to communicate with the built Serverless API. "},{"uri":"https://thanhduy2307.github.io/AWS/5-workshop/5.5-frontend/","title":"Frontend &amp; API Gateway","tags":[],"description":"","content":" üåê Architecture: We will build a standard secure system: Frontend (Amplify) -\u0026gt; API Gateway (with Cognito authentication) -\u0026gt; Lambda Backend.\nConnection Model To ensure security and centralized management, the Frontend is not allowed to call Lambda directly. Instead, we use Amazon API Gateway.\nWorkflow:\nDeploy: The ReactJS Frontend is hosted on AWS Amplify. Authenticate: Users log in via Google/Cognito and receive an IdToken. Request: The Frontend sends a request to API Gateway with the Authorization Header containing the Token. Authorize: API Gateway verifies the Token with Cognito. If valid -\u0026gt; forwards the request to Lambda. Execute: Lambda executes the logic and returns the result. "},{"uri":"https://thanhduy2307.github.io/AWS/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"Throughout my internship at AWS from 08/09/2025 to 12/12/2025, I had the opportunity to learn, train, and apply the knowledge acquired at university to a real-world work environment. I participated in developing and deploying a complete Serverless web application on AWS Cloud, including setting up the access flow (Route 53, CloudFront), security (API Gateway, Cognito), backend logic (Lambda, DynamoDB), and automating the deployment process (CloudFormation, CodePipeline)., thereby improving the following skills: Serverless Programming (AWS Lambda), Serverless Architecture Design, CI/CD Pipeline Setup (with CloudFormation, CodeBuild), NoSQL Optimization (DynamoDB), and REST API Design (API Gateway).. Regarding my professional conduct, I always strived to complete tasks well, comply with regulations, and actively communicate with colleagues to enhance work efficiency.\nTo objectively reflect on my internship process, I would like to self-assess based on the criteria below:\nNo. Criteria Description Excellent (T·ªët) Good (Kh√°) Average (Trung b√¨nh) 1 Knowledge and Technical Skills Understanding of the industry, practical application of knowledge, tool proficiency, quality of work ‚úÖ ‚òê ‚òê 2 Ability to Learn Acquiring new knowledge, fast learning pace ‚úÖ ‚òê ‚òê 3 Proactiveness Self-learning, taking on tasks without waiting for instructions ‚úÖ ‚òê ‚òê 4 Sense of Responsibility Completing work on time, ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Adherence to timing, internal rules, and work processes ‚úÖ ‚òê ‚òê 6 Drive for Improvement Willingness to receive feedback and self-improve ‚úÖ ‚òê ‚òê 7 Communication Presenting ideas, reporting work clearly ‚òê ‚úÖ ‚òê 8 Team Collaboration Working effectively with colleagues, participating in the team ‚úÖ ‚òê ‚òê 9 Professional Conduct Respect for colleagues, partners, and the work environment ‚úÖ ‚òê ‚òê 10 Problem-Solving Thinking Identifying issues, proposing solutions, creativity ‚òê ‚úÖ ‚òê 11 Contribution to Project/Organization Work effectiveness, improvement initiatives, team recognition ‚úÖ ‚òê ‚òê 12 Overall Assessment General evaluation of the entire internship process ‚úÖ ‚òê ‚òê Areas for Improvement Improve communication skills to present ideas and report work more clearly and coherently for each team member. Improve the approach to problem-solving thinking. "},{"uri":"https://thanhduy2307.github.io/AWS/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Complete Frontend‚ÄìBackend Serverless Integration: Understand how to build the Frontend interface and implement API calls from the Frontend to API Gateway. Set Up System Monitoring: Practice creating and using Amazon CloudWatch to build dashboards that monitor system performance. Prepare for the Final Project: Finalize the project idea, assign tasks among team members, and begin building the core components of the project. Tasks for the Week: Day Task Start Date Completion Date Reference 2 - Serverless: Guide to building Frontend that calls API Gateway 13/10/2025 13/10/2025 https://000079.awsstudygroup.com/vi/ 3 - Create system monitoring dashboards with Amazon CloudWatch 14/10/2025 14/10/2025 https://000008.awsstudygroup.com/vi/ 4 - Rest day 15/10/2025 15/10/2025 5 - Team meeting to prepare and finalize final project ideas 16/10/2025 16/10/2025 6 - Practice: Start building the project 17/10/2025 17/10/2025 Week 6 Achievements: Completed Serverless Frontend Integration: Successfully studied and practiced building the Frontend (UI) capable of sending API requests to API Gateway (following the provided guide). Set Up System Monitoring: Practiced creating and configuring CloudWatch Dashboards to monitor basic system resources and performance indicators. Kick-off for the Final Project: Participated in a team meeting to discuss and finalize the idea for the final project. Started building the initial components of the project (e.g., folder structure, initial AWS service setup). Strengthened Serverless Knowledge: Gained a clearer understanding of Serverless architecture flow, especially the interaction between Frontend, API Gateway, and Backend services. Improved Team Collaboration Skills: Actively contributed to discussions and task planning within the team in preparation for the major project. "},{"uri":"https://thanhduy2307.github.io/AWS/7-feedback/","title":"Feedback and Suggestions","tags":[],"description":"","content":" Here you are free to share your personal opinions and experiences about participating in the First Cloud Journey program, helping the FCJ team improve any shortcomings based on the following categories:\nGeneral Assessment 1. Work Environment The work environment is very friendly and open. FCJ team members are always ready to assist when I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, which helps me concentrate better.\n2. Support from Mentor / Team Admin The Mentor provided very detailed guidance, gave clear explanations when I did not understand, and always encouraged me to ask questions. Team admin supported procedures, documentation, and facilitated a smooth working environment for me. The supporting admins were always enthusiastic in reviewing my issues and providing timely solutions.\n3. Alignment between Work and Academic Major The work assigned to me was suitable for the knowledge I acquired at university, while also expanding into new areas that I had not previously encountered. Thanks to this, I both consolidated my foundational knowledge and learned practical skills.\n4. Opportunities for Learning \u0026amp; Skill Development During the internship, I learned many new skills, such as using project management tools, teamwork skills, and professional communication in a corporate environment. The Mentor also shared practical experiences that helped me better define my career direction.\n5. Culture \u0026amp; Team Spirit The company culture is very positive: everyone respects each other, works seriously but remains cheerful. When facing urgent projects, everyone worked together, providing support regardless of position. This made me feel like a part of the collective, even as an intern.\n6. Intern Policies / Benefits The company facilitated flexible working hours when necessary. Furthermore, the opportunity to participate in various events was a major plus.\n"},{"uri":"https://thanhduy2307.github.io/AWS/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Advanced Review: Complete the in-depth review of the 4 pillars of the AWS Well-Architected Framework to prepare for the midterm exam. Knowledge Systematization: Understand the roles and operational mechanisms of the key services (IAM, KMS, Multi-AZ, Lambda, CloudFront) within each pillar. Mindset \u0026amp; Preparation: Maintain a relaxed mindset and be ready for the midterm exam. Tasks to Complete This Week: Day Tasks Start Date End Date Resources 2 - Review Secure Architectures: IAM, KMS, Security Groups, NACLs, Secrets Manager 20/10/2025 20/10/2025 3 - Review Resilient Architectures: Multi-AZ, Auto Scaling, Route 53, DR Strategies 21/10/2025 21/10/2025 4 - Review High-Performing Architectures: Lambda, Caching, CloudFront, EC2 Auto Scaling, S3 (Storage Tiering). 22/10/2025 22/10/2025 5 - Continue reviewing High-Performing Architectures: Lambda, Caching, CloudFront, EC2 Auto Scaling, S3 (Storage Tiering). 23/10/2025 23/10/2025 6 - Rest and prepare mentally for the midterm exam 24/10/2025 24/10/2025 Week 7 Achievements: Completed Review Plan: Completed the in-depth review of all 4 pillars of the AWS Well-Architected Framework (Security, Reliability, Performance Efficiency, Cost Optimization) according to the schedule.\nKnowledge Systematization: Successfully understood and categorized the roles of the core services (IAM, KMS, Multi-AZ, Auto Scaling, Lambda, CloudFront, S3) within each architectural pillar.\nFoundation Strengthening: Reviewed fundamental AWS service groups (Compute, Storage, Networking, Database, \u0026hellip;) and practiced using AWS Console/CLI.\nExam Preparation: Summarized and systematized knowledge, and stayed mentally prepared for the midterm exam.\n"},{"uri":"https://thanhduy2307.github.io/AWS/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference only. Please do not copy it verbatim for your own report, including this warning.\nWeek 8 Goals: Start implementing the actual project using AWS serverless services. Build a basic backend workflow using DynamoDB, Lambda, and API Gateway. Register a domain and configure DNS via Route 53 for API exposure. Tasks for This Week: Day Tasks Start Date Completion Date Reference 2 - Analyze project requirements - Define backend workflow: API Gateway ‚Üí Lambda ‚Üí DynamoDB 27/10/2025 27/10/2025 3 - Create DynamoDB Table + Define Partition Key + Insert sample items 28/10/2025 28/10/2025 4 - Register domain via Route 53 + Purchase domain + Create Public Hosted Zone + Add A/CNAME records for routing setup 29/10/2025 29/10/2025 5 - Write Lambda functions for API + Implement CRUD operations with DynamoDB SDK + Create IAM Role for Lambda 30/10/2025 30/10/2025 6 - Create API Gateway \u0026amp; integrate with Lambda + Set up REST API + Add GET/POST methods + Deploy stage + Test endpoint 31/10/2025 31/10/2025 Week 8 Achievements: Successfully created the DynamoDB Table for the project:\nDefined the Partition Key Added sample data to test workflow Verified functionality via Query/Scan Completed Route 53 domain registration, including:\nCreating a Public Hosted Zone Adding DNS records (A, CNAME) Verifying DNS propagation Built and deployed an AWS Lambda function:\nImplemented logic for GET/POST requests Integrated DynamoDB SDK for read/write operations Configured appropriate IAM Role permissions Set up API Gateway:\nCreated REST API with resources and methods Integrated with Lambda using Lambda Proxy Deployed the dev stage Successfully tested API endpoint Completed full serverless workflow integration: Client ‚Üí API Gateway ‚Üí Lambda ‚Üí DynamoDB\nStrengthened understanding of real-world serverless architecture and how services tie together to form a working backend.\n‚Ä¶\n"},{"uri":"https://thanhduy2307.github.io/AWS/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference only. Please do not copy it verbatim into your report, including this warning.\nWeek 9 Goals: Integrate Google Authentication into Amazon Cognito (Google as an Identity Provider). Configure Amazon SES to send emails (notifications, verification, OTP). Test full flow: user login ‚Üí Cognito authentication ‚Üí SES email delivery. Tasks for This Week: Day Tasks Start End Reference 2 - Prepare Cognito environment - Create new User Pool \u0026amp; App Client for Google Login 03/11/2025 03/11/2025 3 - Configure Google OAuth: + Create OAuth Client ID in GCP + Retrieve Client ID \u0026amp; Secret 04/11/2025 04/11/2025 4 - Integrate Google with Cognito: + Create Google Identity Provider + Attribute Mapping + Test login flow 05/11/2025 05/11/2025 5 - Set up Amazon SES: + Domain verification + Sender email verification + DKIM setup + Remove SES Sandbox limitation 06/11/2025 06/11/2025 6 - Build SES email Lambda + Integrate with API Gateway + Test sending email via REST API 07/11/2025 07/11/2025 Week 9 Achievements: Successfully integrated Google Login with Amazon Cognito:\nCreated User Pool \u0026amp; App Client Set up Google OAuth in GCP Added Google as an Identity Provider in Cognito Verified login \u0026amp; token exchange workflow Fully configured Amazon SES:\nDomain and email verification DKIM enabled for improved email deliverability SES successfully moved out of sandbox Able to send verified emails Built a Lambda function for email sending via SES:\nSupports OTP / verification / notification emails Integrated with API Gateway for external triggering Verified entire authentication + email workflow: Google Sign-in ‚Üí Cognito Authentication ‚Üí Lambda ‚Üí SES Email Delivery\nImproved understanding of OAuth provider integration \u0026amp; email service configuration within AWS.\n‚Ä¶\n"},{"uri":"https://thanhduy2307.github.io/AWS/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Goals: Find an alternative email solution after SES sandbox removal request was rejected. Switch to Resend as the primary email delivery service. Configure API authentication (API Key / JWT) to protect the email endpoint. Rebuild the email workflow using Resend. Tasks for This Week: Day Tasks Start End Reference 2 - Received SES sandbox removal rejection - Evaluate project email requirements and possible alternatives 10/11/2025 10/11/2025 3 - Research Resend: + Generate API Key + Domain verification + Explore email API usage 11/11/2025 11/11/2025 https://resend.com/docs 4 - Integrate Resend with Lambda: + Use SDK/HTTP API + Create email templates + Test email sending 12/11/2025 12/11/2025 5 - Configure API Authentication: + API Key or + JWT via Cognito + Enable auth in API Gateway 13/11/2025 13/11/2025 6 - End-to-End Testing: + Client ‚Üí API Gateway ‚Üí Lambda ‚Üí Resend + Validate tokens + Debug logs in CloudWatch 14/11/2025 14/11/2025 CloudWatch Logs Week 10 Achievements: SES sandbox removal request was rejected, causing limitations:\nUnable to send emails to unverified addresses Not suitable for production usage Required switching to an external provider Successfully migrated to Resend:\nCreated API Key Verified domain for better deliverability Sent emails successfully using Resend API Integrated Lambda + Resend:\nSupports HTML templates Email types: OTP, verification, notification (Optional) Stored API Key securely in AWS Secrets Manager Implemented API Authentication:\nAPI Key or JWT Cognito authentication API Gateway now requires authentication to trigger Lambda Improved security of email API endpoint Completed full workflow testing: Client ‚Üí Authenticated API Gateway ‚Üí Lambda ‚Üí Resend ‚Üí Email Delivered\nStrengthened understanding of external email providers and API authentication methods.\n‚Ä¶\n"},{"uri":"https://thanhduy2307.github.io/AWS/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The content below is for reference only. Please do not copy it verbatim into your report.\nWeek 11 Goals: Integrate frontend with backend using AWS Amplify. Use Cognito for authentication (login, JWT, API Key) in frontend. Integrate Lambda + Resend + API Gateway into frontend via Amplify API. Deploy full CI/CD automatically via Amplify Console. Tasks for This Week: Day Tasks Start End Reference 2 - Configure Amplify project + amplify init + Connect frontend (React/Vue/Angular) + Add Amplify Auth (Cognito User Pool) 17/11/2025 17/11/2025 AWS Amplify Docs 3 - Integrate Lambda + Resend API via Amplify API + Create REST/GraphQL endpoint + Configure authentication using Cognito JWT/API Key 18/11/2025 18/11/2025 Amplify Docs, Cognito Docs 4 - Test frontend ‚Üí Amplify API ‚Üí Lambda ‚Üí Resend + Verify Cognito login works + Send OTP/notification emails from frontend 19/11/2025 19/11/2025 Project Docs 5 - Deploy CI/CD via Amplify Console + Connect GitHub/GitLab/CodeCommit repo + Auto build \u0026amp; deploy frontend + backend Lambda 20/11/2025 20/11/2025 Amplify Console Docs 6 - Test end-to-end pipeline + Push code ‚Üí Amplify auto deploy + Check frontend UI, API authentication, and email sending via Resend 21/11/2025 21/11/2025 CloudWatch Logs Week 11 Achievements: Fully integrated frontend with backend via Amplify:\nCall Lambda + Resend API from frontend Cognito authentication (login, JWT, API Key) works OTP/verification/notification emails sent successfully Deployed fully automated CI/CD via Amplify Console:\nRepo as source (GitHub/GitLab/CodeCommit) Auto build \u0026amp; deploy frontend + backend Production updated automatically on push End-to-end workflow tested: Frontend ‚Üí Cognito Auth ‚Üí Amplify API ‚Üí Lambda ‚Üí Resend ‚Üí Email ‚Üí Frontend displays notification\nLearned how to integrate serverless full-stack with Amplify + Cognito + Lambda + Resend and CI/CD automation.\n‚Ä¶\n"},{"uri":"https://thanhduy2307.github.io/AWS/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Review and re-check the entire project before submission. Complete the proposal document and materials for the workshop. Prepare the presentation slides for the final report. Tasks Completed This Week: Day Task Start Finish Source 2 - Reviewed the entire source code + Checked logic + Verified UI/UX + Validated backend‚Äìfrontend flow 24/11/2025 24/11/2025 ‚Äî 3 - Finalized the proposal + Wrote system description + Refined the overall architecture section 25/11/2025 25/11/2025 ‚Äî 4 - Prepared workshop content + Technical slides + Functionality demo 26/11/2025 26/11/2025 Workshop Notes 5 - Designed presentation slides + Optimized content + Standardized illustrations and graphics 27/11/2025 27/11/2025 ‚Äî Week 12 Achievements: The entire project was reviewed and checked for issues:\nAll features are running smoothly No major bugs found Demo runs flawlessly for workshop and presentation The proposal was fully completed:\nClear structure Comprehensive system description, architecture, and workflow Formatted according to course requirements Workshop materials were prepared:\nTechnical explanation slides Complete demo flow Presentation slides were finalized:\nClear, concise content Consistent visuals and formatting Ready for the final presentation "},{"uri":"https://thanhduy2307.github.io/AWS/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://thanhduy2307.github.io/AWS/tags/","title":"Tags","tags":[],"description":"","content":""}]